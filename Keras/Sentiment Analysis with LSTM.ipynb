{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwnBEK_tUu37"
      },
      "source": [
        "**Initialization**\n",
        "* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a same Project or Problem. And the third line of code helps to make visualization within the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1f6d_sYUaEg"
      },
      "source": [
        "#@ Initialization:\n",
        "%reload_ext autoreload \n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmUuVZUGWdPY"
      },
      "source": [
        "**Downloading the Dependencies**\n",
        "* I have downloaded all the Libraries and Dependencies required for this Project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-Jf1iQ0WawX"
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies:\n",
        "import os, glob\n",
        "from random import shuffle\n",
        "from IPython.display import display\n",
        "\n",
        "import numpy as np                                      # Module to work with Arrays.\n",
        "from keras.preprocessing import sequence                # Module to handle Padding Input.\n",
        "from keras.models import Sequential                     # Base Keras Neural Network Model.\n",
        "from keras.layers import Dense, Dropout, Flatten        # Layers Objects to pile into Model.\n",
        "from keras.layers import LSTM                           # Convolutional Layer and MaxPooling.\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer         # Module for Tokenization.\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ewZv7vXpTO"
      },
      "source": [
        "**Getting the Data**\n",
        "* I have used Google Colab for this Project so the process of downloading and reading the Data might be different in other platforms. I have used [**Large Moview Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/) for this Project. This is a dataset for binary sentiment classification containing substantially more data. The Dataset has a set of 25,000 highly polar movie reviews for training and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-EISkhKXiwC"
      },
      "source": [
        "#@ Getting the Data:\n",
        "def preprocess_data(filepath):\n",
        "  positive_path = os.path.join(filepath, \"pos\")\n",
        "  negative_path = os.path.join(filepath, \"neg\")\n",
        "  pos_label = 1\n",
        "  neg_label = 0\n",
        "  dataset = []\n",
        "  \n",
        "  for filename in glob.glob(os.path.join(positive_path, '*.txt')):                            # Positive Sentiment Dataset.\n",
        "    with open(filename, \"r\") as f:\n",
        "      dataset.append((pos_label, f.read()))\n",
        "  for filename in glob.glob(os.path.join(negative_path, '*.txt')):                            # Negative Sentiment Dataset.\n",
        "    with open(filename, \"r\") as f:\n",
        "      dataset.append((neg_label, f.read()))\n",
        "\n",
        "  shuffle(dataset)                                                                            # Shuffling the Dataset.\n",
        "  return dataset "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuxpbVP66HXq"
      },
      "source": [
        "**Processing the Dataset**\n",
        "* I have manually downloaded the Dataset from [**Large Moview Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/). I have used the small subset of Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMZARJrpYS6X",
        "outputId": "e16998d2-3771-44aa-cdbc-0f55b155d093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#@ Processing the Dataset:\n",
        "PATH = \"/content/drive/My Drive/Colab Notebooks/Data/Smalltrain\"                     # Path to the Dataset.\n",
        "dataset = preprocess_data(PATH)                                                      # Processing the Dataset.\n",
        "\n",
        "#@ Inspecting the Dataset:\n",
        "dataset[:3]                                                                          # Inspecting the Dataset."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  'Admittedly, I find Al Pacino to be a guilty pleasure. He was a fine actor until Scent of a Woman, where he apparently overdosed on himself irreparably. I hoped this film, of which I\\'d heard almost nothing growing up, would be a nice little gem. An overlooked, ahead-of-its-time, intelligent and engaging city-political thriller. It\\'s not.<br /><br />City Hall is a movie that clouds its plot with so many characters, names, and \"realistic\" citywide issues, that for a while you think its a plot in scope so broad and implicating, that once you find out the truth, it will blow your mind. In truth, however, these subplots and digressions result ultimately in fairly tame and very familiar urban story trademarks such as Corruption of Power, Two-Faced Politicians, Mafia with Police ties, etc. And theoretically, this setup allows for some thrilling tension, the fear that none of the characters are safe, and anything could happen! But again, it really doesn\\'t.<br /><br />Unfortunately, the only things that happen are quite predictable, and we\\'re left with several \"confession\" monologues, that are meant as a whole to form modern a fable of sorts, a lesson in the moral ambiguity of the \"real world\" of politics and society. But after 110 minutes of names and missing reports and a spider-web of lies and cover-ups, the audience is usually treated to a somewhat satisfying reveal. I don\\'t think we\\'re left with that in City Hall, and while it\\'s a very full film, I don\\'t find it altogether rich.'),\n",
              " (0,\n",
              "  'The Biggest one that bugs the hell out of me is that they say Zues takes DUTCH commands. But she is speaking German to him. The 2 languishes are completely different, its like saying \"well he takes French commands\" and start talking Spanish.<br /><br />James Belushi gives more the feeling of being a comedy actor not a detective in the slightest. The role just doesn\\'t fit him, even if its mend to be a comedy.<br /><br />To many stereotype/predicable stuff. Typical comment or comebacks.<br /><br />If you don\\'t look at those things i think it could be a nice movie to watch if its ever on TV. But i wouldn\\'t suggesting renting it.'),\n",
              " (0,\n",
              "  'One has to wonder if at any point in the production of this film a<br /><br />script existed that made any sense. Was the rough cut 3 hours<br /><br />long and was it trimmed into the incoherent mess that survives? <br /><br />Why would anyone finance this mess? I will say that Tom<br /><br />Wlaschiha is a good looking young man and he does what he can<br /><br />with the dialogue and dramatic (?) situations he is given. But<br /><br />characters come and go for no apparent reason, continuity is<br /><br />non-existent, and the acting, cinematography, and direction are (to<br /><br />put it politely) amateurish. Not One Sleeps is an unfortunate<br /><br />choice of title as it will probably prove untrue should anyone<br /><br />actually attempt to actually watch this film.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKFJfODv6QZc"
      },
      "source": [
        "**Tokenization and Vectorization**\n",
        "* The next step is to perform the Tokenization and Vectorization of the Dataset. I will use Google news pretrained Model Vectors for the process of Vectorization. The Google News Word2vec Vocabulary includes some stopwords as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unkG86T46Mtr",
        "outputId": "89f34102-7c68-469c-e7fc-54319dad617c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#@ Tokenization and Vectorization:\n",
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"                # Pretrained Word2vec Model.    \n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format(\"/content/GoogleNews-vectors-negative300.bin.gz\",           # Word2vec Model Vectors.\n",
        "                                       binary=True, limit=100000)\n",
        "\n",
        "#@ Function for Tokenization and Vectorization:\n",
        "def tokenize_and_vectorize(dataset):\n",
        "  tokenizer = TreebankWordTokenizer()                                  # Instantiating the Tokenizer.\n",
        "  vectorized_data = []\n",
        "  for sample in dataset:\n",
        "    tokens = tokenizer.tokenize(sample[1])                             # Process for Tokenization.\n",
        "    sample_vecs = []\n",
        "    for token in tokens:\n",
        "      try:\n",
        "        sample_vecs.append(word_vectors[token])                        # Process for Vectorization.\n",
        "      except KeyError:\n",
        "        pass\n",
        "    vectorized_data.append(sample_vecs)\n",
        "  \n",
        "  return vectorized_data                                               # Returning the Vectorized Data.\n",
        "\n",
        "#@ Function for Collecting the Target Labels:\n",
        "def collect_expected(dataset):\n",
        "  \"\"\" Collecting the Target Labels: 0 for Negative Review and 1 for Positive Review. \"\"\"\n",
        "  expected=[]\n",
        "  for sample in dataset:\n",
        "    expected.append(sample[0])\n",
        "  return expected\n",
        "\n",
        "#@ Tokenization and Vectorization:\n",
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHWX1QW06gys"
      },
      "source": [
        "**Splitting into Training and Testing.**\n",
        "* Now, I will split the above obtained Dataset into Training set and a Test set. I will split the Dataset into 80% for Training and 20% for Test set. The next code will bucket the Data into Training set X train along with correct labels y train and similarly into Test set X test along with correct labels y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGkSY4Sg6k9q"
      },
      "source": [
        "#@ Splitting the Dataset into Training set and Test set:\n",
        "split_part = int(len(vectorized_data) * 0.8)\n",
        "\n",
        "#@ Training set:\n",
        "X_train = vectorized_data[:split_part]\n",
        "y_train = expected[:split_part]\n",
        "\n",
        "#@ Test set:\n",
        "X_test = vectorized_data[split_part:]\n",
        "y_test = expected[split_part:]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z09tBA5h8ioP"
      },
      "source": [
        "### **Long Short Term Memory**\n",
        "* Long Short Term Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of Deep Learning. Unlike standard Feedforward Neural Networks, LSTM has Feedback connections. It can not only process single data points, but also entire sequences of data such as Speech or Video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvdINdSr9LCC"
      },
      "source": [
        "#@ Parameters of LSTM Neural Network:\n",
        "maxlen = 500                                    # Maximum review length.\n",
        "batch_size = 32                                 # Number of samples shown to the network before updating the weights.\n",
        "embedding_dims = 300                            # Length of token vectors for passing in RNN.\n",
        "epochs = 10                                     # Number of times for passing the training dataset.\n",
        "num_neurons = 50"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPkqfOR-_rZZ"
      },
      "source": [
        "**Padding and Truncating the Sequence**\n",
        "* **Keras** has the preprocessing helper method called pad_sequences which is used to pad the input Data. But it works only on the sequence of scalars and sequence of vectors. Now, I will write the helper function to pad the input Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsZGn2ag_nMY",
        "outputId": "161a8ec8-d08d-4e70-8032-be3597b19a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#@ Padding and Truncating the Token Sequence:\n",
        "\n",
        "def pad_trunc(data, maxlen):\n",
        "  \"\"\" Padding the Dataset with zero Vectors. \"\"\"\n",
        "  new_data = []\n",
        "  # Creating zeros vectors of length of Word vectors.\n",
        "  zero_vector = []\n",
        "  for _ in range(len(data[0][0])):\n",
        "    zero_vector.append(0.0)\n",
        "\n",
        "  for sample in data:\n",
        "    if len(sample) > maxlen:\n",
        "      temp = sample[:maxlen]\n",
        "    elif len(sample) < maxlen:\n",
        "      temp = sample \n",
        "      # Append the appropriate number of 0 vectors to the list.\n",
        "      additional_elems = maxlen - len(sample)\n",
        "      for _ in range(additional_elems):\n",
        "        temp.append(zero_vector)\n",
        "    else:\n",
        "      temp = sample \n",
        "    new_data.append(temp)\n",
        "  return new_data\n",
        "\n",
        "\n",
        "#@ Gathering the Truncated and Augmented Data:\n",
        "X_train = pad_trunc(X_train, maxlen)\n",
        "X_test = pad_trunc(X_test, maxlen)\n",
        "\n",
        "#@ Converting the Data into Numpy Arrays:\n",
        "X_train = np.reshape(X_train, (len(X_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.reshape(X_test, (len(X_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "#@ Inspecting the shape of the Data:\n",
        "display(f\"Shape of Training Data {X_train.shape, y_train.shape}\")\n",
        "display(f\"Shape of Testing Data {X_test.shape, y_test.shape}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Shape of Training Data ((1601, 500, 300), (1601,))'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Shape of Testing Data ((401, 500, 300), (401,))'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzppEIiR_3U_"
      },
      "source": [
        "**Long Short Term Memory**\n",
        "* Now, The Dataset is ready to build the Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcbc9E4q_zp-",
        "outputId": "b849a5e3-8b9b-4347-f1a1-941283539ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "#@ Long Short Term Memory or LSTM:\n",
        "model = Sequential()                                     # Standard Model Definition for Keras.\n",
        "model.add(LSTM(                                          # Adding the LSTM Layer.\n",
        "    num_neurons, return_sequences=True,\n",
        "    input_shape=(maxlen, embedding_dims)\n",
        "))\n",
        "model.add(Dropout(0.2))                                  # Adding the Dropout Layer.\n",
        "model.add(Flatten())                                     # Flatten the output of LSTM.\n",
        "model.add(Dense(1, activation=\"sigmoid\"))                # Output Layer.\n",
        "\n",
        "#@ Compiling the LSTM Neural Network:\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"rmsprop\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#@ Training the LSTM Neural Network:\n",
        "model.fit(\n",
        "    X_train, y_train,                                     # Training Dataset.\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_test, y_test)                      # Validation Dataset.\n",
        ")\n",
        "\n",
        "#@ Inspecting the Summary of the Model:\n",
        "print(\"\\n\")\n",
        "model.summary()                                           # Summary of the Model."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "51/51 [==============================] - 2s 45ms/step - loss: 0.6623 - accuracy: 0.5909 - val_loss: 0.6957 - val_accuracy: 0.5511\n",
            "Epoch 2/10\n",
            "51/51 [==============================] - 2s 31ms/step - loss: 0.5012 - accuracy: 0.7733 - val_loss: 0.8568 - val_accuracy: 0.5536\n",
            "Epoch 3/10\n",
            "51/51 [==============================] - 2s 31ms/step - loss: 0.4048 - accuracy: 0.8264 - val_loss: 0.8725 - val_accuracy: 0.6384\n",
            "Epoch 4/10\n",
            "51/51 [==============================] - 2s 30ms/step - loss: 0.3380 - accuracy: 0.8601 - val_loss: 1.0802 - val_accuracy: 0.5561\n",
            "Epoch 5/10\n",
            "51/51 [==============================] - 2s 30ms/step - loss: 0.2907 - accuracy: 0.8957 - val_loss: 0.6979 - val_accuracy: 0.6683\n",
            "Epoch 6/10\n",
            "51/51 [==============================] - 2s 30ms/step - loss: 0.2311 - accuracy: 0.9163 - val_loss: 0.5331 - val_accuracy: 0.7880\n",
            "Epoch 7/10\n",
            "51/51 [==============================] - 2s 30ms/step - loss: 0.1869 - accuracy: 0.9282 - val_loss: 0.7506 - val_accuracy: 0.7132\n",
            "Epoch 8/10\n",
            "51/51 [==============================] - 2s 30ms/step - loss: 0.1548 - accuracy: 0.9407 - val_loss: 1.0031 - val_accuracy: 0.6858\n",
            "Epoch 9/10\n",
            "51/51 [==============================] - 2s 31ms/step - loss: 0.1184 - accuracy: 0.9525 - val_loss: 0.6835 - val_accuracy: 0.7656\n",
            "Epoch 10/10\n",
            "51/51 [==============================] - 2s 30ms/step - loss: 0.0758 - accuracy: 0.9813 - val_loss: 0.7657 - val_accuracy: 0.7706\n",
            "\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 500, 50)           70200     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 500, 50)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 25001     \n",
            "=================================================================\n",
            "Total params: 95,201\n",
            "Trainable params: 95,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRMfn59SILFB"
      },
      "source": [
        "**Saving the LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFLf9fNdHJjv",
        "outputId": "308681e3-c6eb-4ac4-9979-75438ecc02e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@ Saving the Recurrent Neural Network:\n",
        "model_structure = model.to_json()\n",
        "with open(\"lstm.json\", \"w\") as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights(\"lstm.h5\")\n",
        "print(\"Model saved!!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsYE1FBdICLk"
      },
      "source": [
        "**Model Evaluation**\n",
        "* Now, I have trained a Model. I will make a sentence with Positive Sentiment and I will predict the Sentiment of the sentence using the Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dshryZ72IDVi",
        "outputId": "20455574-3714-414d-95a3-c9454490a1fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@ Model Evaluation:\n",
        "sample_1 = \"\"\" I hate that the dismal weather had me down for so long, \\ \n",
        "            when will it break! Ugh, when does happiness return? The sun is \\ \n",
        "            blinding and the puffy clouds are too thin. I can't wait for the weekend.\"\"\"\n",
        "\n",
        "#@ Making Predictions:\n",
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "#@ Inspecting the Prediction:\n",
        "f\"The predicted sentiment by the Model is: {model.predict_classes(test_vec)}\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The predicted sentiment by the Model is: [[0]]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNvwiT98KxE4"
      },
      "source": [
        "**Optimizing the Vector Size**\n",
        "* Padding and Truncating each sample to 400 Tokens was important for Convolutional Neural Nets so that the filters could scan a vector with a consistent length. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndWwMXmTI6cP",
        "outputId": "f3b0fd8d-1f17-4a7e-e899-29fe9a8c34e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#@ Optimizing the Vector Size:\n",
        "def test_len(data, maxlen):\n",
        "  total_len = truncated = exact = padded = 0\n",
        "  for sample in data:\n",
        "    total_len = len(sample)\n",
        "    if len(sample) > maxlen:\n",
        "      truncated += 1\n",
        "    elif len(sample) < maxlen:\n",
        "      padded += 1\n",
        "    else:\n",
        "      exact += 1\n",
        "  print(f\"Padded: {padded}\")\n",
        "  print(f\"Equal: {exact}\")\n",
        "  print(f\"Truncated: {truncated}\")\n",
        "  print(f\"Average length: {total_len/len(data)}\")\n",
        "\n",
        "#@ Applying in the Dataset:\n",
        "test_len(vectorized_data, 500)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padded: 0\n",
            "Equal: 1897\n",
            "Truncated: 105\n",
            "Average length: 0.24975024975024976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8mVxmxMOqzh"
      },
      "source": [
        "**Optimized Long Short Term Memory**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2etJi8fPQSt"
      },
      "source": [
        "#@ Parameters of LSTM Neural Network:\n",
        "maxlen = 200                                    # Maximum review length.\n",
        "batch_size = 32                                 # Number of samples shown to the network before updating the weights.\n",
        "embedding_dims = 300                            # Length of token vectors for passing in RNN.\n",
        "epochs = 10                                     # Number of times for passing the training dataset.\n",
        "num_neurons = 50\n",
        "\n",
        "#@ Gathering the Truncated and Augmented Data:\n",
        "X_train = pad_trunc(X_train, maxlen)\n",
        "X_test = pad_trunc(X_test, maxlen)\n",
        "\n",
        "#@ Converting the Data into Numpy Arrays:\n",
        "X_train = np.reshape(X_train, (len(X_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.reshape(X_test, (len(X_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbRanSMONnye",
        "outputId": "bd7b5b13-ae8f-492d-933b-29c7badd2bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "#@ Long Short Term Memory or LSTM:\n",
        "model = Sequential()                                     # Standard Model Definition for Keras.\n",
        "model.add(LSTM(                                          # Adding the LSTM Layer.\n",
        "    num_neurons, return_sequences=True,\n",
        "    input_shape=(maxlen, embedding_dims)\n",
        "))\n",
        "model.add(Dropout(0.2))                                  # Adding the Dropout Layer.\n",
        "model.add(Flatten())                                     # Flatten the output of LSTM.\n",
        "model.add(Dense(1, activation=\"sigmoid\"))                # Output Layer.\n",
        "\n",
        "#@ Compiling the LSTM Neural Network:\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"rmsprop\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#@ Training the LSTM Neural Network:\n",
        "model.fit(\n",
        "    X_train, y_train,                                     # Training Dataset.\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_test, y_test)                      # Validation Dataset.\n",
        ")\n",
        "\n",
        "#@ Inspecting the Summary of the Model:\n",
        "print(\"\\n\")\n",
        "model.summary()                                           # Summary of the Model."
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.6615 - accuracy: 0.5915 - val_loss: 0.6577 - val_accuracy: 0.5761\n",
            "Epoch 2/10\n",
            "51/51 [==============================] - 1s 15ms/step - loss: 0.5017 - accuracy: 0.7695 - val_loss: 0.7457 - val_accuracy: 0.6359\n",
            "Epoch 3/10\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.4427 - accuracy: 0.7983 - val_loss: 0.4625 - val_accuracy: 0.7781\n",
            "Epoch 4/10\n",
            "51/51 [==============================] - 1s 15ms/step - loss: 0.3588 - accuracy: 0.8576 - val_loss: 0.5772 - val_accuracy: 0.6983\n",
            "Epoch 5/10\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.3003 - accuracy: 0.8782 - val_loss: 0.5120 - val_accuracy: 0.7706\n",
            "Epoch 6/10\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.2584 - accuracy: 0.8876 - val_loss: 0.5452 - val_accuracy: 0.7456\n",
            "Epoch 7/10\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.1946 - accuracy: 0.9250 - val_loss: 1.4150 - val_accuracy: 0.6110\n",
            "Epoch 8/10\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.1570 - accuracy: 0.9419 - val_loss: 0.5795 - val_accuracy: 0.7656\n",
            "Epoch 9/10\n",
            "51/51 [==============================] - 1s 15ms/step - loss: 0.1187 - accuracy: 0.9625 - val_loss: 0.9093 - val_accuracy: 0.7481\n",
            "Epoch 10/10\n",
            "51/51 [==============================] - 1s 15ms/step - loss: 0.0897 - accuracy: 0.9744 - val_loss: 0.6616 - val_accuracy: 0.7681\n",
            "\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 200, 50)           70200     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 200, 50)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 10001     \n",
            "=================================================================\n",
            "Total params: 80,201\n",
            "Trainable params: 80,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}