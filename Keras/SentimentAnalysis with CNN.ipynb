{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzKV5-2nv4dA"
      },
      "source": [
        "**Initialization**\n",
        "* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a Project or Problem. And the third line of code helps to make visualization within the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj-NI7PivXIP"
      },
      "source": [
        "#@ Initialization:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2 \n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emucWoKGwTKW"
      },
      "source": [
        "**Downloading the Dependencies**\n",
        "* I have downloaded all the Libraries and Dependencies required for this Project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TLnl5GHwQx4"
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies:\n",
        "# !pip install nlpia\n",
        "import os, glob\n",
        "from random import shuffle\n",
        "from IPython.display import display\n",
        "\n",
        "import numpy as np                                      # Module to work with Arrays.\n",
        "from keras.preprocessing import sequence                # Module to handle Padding Input.\n",
        "from keras.models import Sequential                     # Base Keras Neural Network Model.\n",
        "from keras.layers import Dense, Dropout, Activation     # Layers Objects to pile into Model.\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D     # Convolutional Layer and MaxPooling.\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer         # Module for Tokenization.\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from nlpia.loaders import get_data                      # Importing the NLPIA Package."
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlazyhiY1opk"
      },
      "source": [
        "**Getting the Data**\n",
        "* I have used Google Colab for this Project so the process of downloading and reading the Data might be different in other platforms. I have used [**Large Moview Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/) for this Project. This is a dataset for binary sentiment classification containing substantially more data. The Dataset has a set of 25,000 highly polar movie reviews for training and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZSfbSTMy-Nt"
      },
      "source": [
        "#@ Getting the Data:\n",
        "def preprocess_data(filepath):\n",
        "  positive_path = os.path.join(filepath, \"pos\")\n",
        "  negative_path = os.path.join(filepath, \"neg\")\n",
        "  pos_label = 1\n",
        "  neg_label = 0\n",
        "  dataset = []\n",
        "  \n",
        "  for filename in glob.glob(os.path.join(positive_path, '*.txt')):                            # Positive Sentiment Dataset.\n",
        "    with open(filename, \"r\") as f:\n",
        "      dataset.append((pos_label, f.read()))\n",
        "  for filename in glob.glob(os.path.join(negative_path, '*.txt')):                            # Negative Sentiment Dataset.\n",
        "    with open(filename, \"r\") as f:\n",
        "      dataset.append((neg_label, f.read()))\n",
        "\n",
        "  shuffle(dataset)                                                                            # Shuffling the Dataset.\n",
        "  return dataset "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKeAvcUE-h1P"
      },
      "source": [
        "**Processing the Dataset**\n",
        "* I have manually downloaded the Dataset from [**Large Moview Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/). I have used the small subset of Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxusBQ_ba3G-",
        "outputId": "ce5d34d3-e5cf-4c5c-bc30-e6c333f72605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#@ Processing the Dataset:\n",
        "PATH = \"/content/drive/My Drive/Colab Notebooks/Data/Smalltrain\"                     # Path to the Dataset.\n",
        "dataset = preprocess_data(PATH)                                                      # Processing the Dataset.\n",
        "\n",
        "#@ Inspecting the Dataset:\n",
        "dataset[:3]                                                                          # Inspecting the Dataset."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  \"When Marlene Dietrich was labeled box office poison in 1938 one of a handful of actresses so named by the trades papers, it was films like The Garden Of Allah. How a film could be so breathtakingly beautiful to behold and be so insipidly dull is beyond me. Also how Marlene if she was trying to expand her range and not play a sexpot got stuck with such an old fashioned story is beyond me.<br /><br />The Garden Of Allah, one of the very first films in modern technicolor was a novel set at the turn of the last century by Robert Hitchens who then collaborated on a play adaption with Mary Anderson that ran for 241 performances in 1911-12. It then got two silent screen adaptions. The story is about a monk who runs away from the monastery out in French Tunisia to see some of what he's missed in the world. He runs into a similarly sheltered woman who was unmarried and spent her prime years caring for a sick parent. She's traveling now in the desert and the two meet on a train.<br /><br />The woman is of course Marlene and the runaway monk is Charles Boyer. I'm not sure what was in David O. Selznick's mind in filming this story. Someone like Ingrid Bergman might have made it palatable for the audience. But you can bet that the movie-going public of 1936 when they plunked their money down for a ticket they expected to see Marlene as a modern day Salome rather than a saint with that title. The public still remembered Rudolph Valentino and you can bet that it was some desert romance and seduction that they were expecting.<br /><br />As for the monks you have to remember that they are self supporting in their monasteries and this particular one bottles a special wine of which Boyer happens to be the one with the secret. The monastery will have to rethink it's economics if Boyer leaves. The monks are a sincerely pious group, but from the head man Charles Waldron on down they've a right to be a little concerned with some self interest.<br /><br />Anyway a whole lot of religious platitudes get said here by a pair of leads that really are not suited for the parts. Most especially Marlene Dietrich. I would watch this film with an eye for the special color desert cinematography and forget the plot.\"),\n",
              " (0,\n",
              "  'I\\'m trying to picture the pitch for Dark Angel. \"I\\'m thinking Matrix, I\\'m thinking Bladerunner, I\\'m thinking that chick that plays Faith in Angel, wearing shiny black leather - or some chick just like her, leave that one with us. Only - get this! - we\\'ll do it without any plot, dialogue, character, decent action or budget, just some loud bangs and a hot chick in shiny black leather straddling a big throbbing bike. Fanboys dig loud bangs and hot chicks in shiny black leather straddling big throbbing bikes, right?\"<br /><br />Flashy, shallow, dreary, formulaic, passionless, tedious, dull, dumb, humourless, desultory, barely competent. Live action anime without any action, or indeed any life. SF just the way Joe Fanboy likes it, in fact. :('),\n",
              " (0,\n",
              "  'I bought this adaptation because I really liked Anne BrontÃ«\\'s novel when I read it some time ago and usually particularly enjoy BBC dramas. But I\\'m very disappointed, I never thought it would be as bad as that: the whole series made me laugh much more than moved me as the novel had.<br /><br />First of all, the music (and songs) seems totally out of place in a period drama (sounds as if it\\'s been written for a contemporary horror film)and like another commentator, I was particularly annoyed by the way the cameras spun and spun round the actors. I\\'ve seen some scenes filmed that way in \"North and South\" and it seemed all right there but in The Tenant, it\\'s definitely overdone and simply annoying. Camera movements cannot make wooden acting lively.<br /><br />Most of the second roles were difficult to distinguish at first and the script lacked clarity. None of the characters were properly introduced at first. The little boy gave a very good performance, he\\'s very cute and the best feature of the film.<br /><br />SPOILERS Tara Fitzgerald\\'s characterisation of Helen Graham made her appear cold and harsh, letting no emotion pass through. She doesn\\'t seem to be able to cry at all in a realistic way. I just couldn\\'t believe Markham could have fell for her and I\\'m not mentioning the awful hairdo she was given. I could not help feeling some sympathy with her husband! Fancy being married to such a virago... Besides, he was the only main actor that sounded right to me. Toby Stephens I found just OK, Helen Graham\\'s brother not very good. <br /><br />Maybe it\\'s difficult to adapt a novel that deals with such bleak subjects as alcoholism and cruelty. Besides, what is only hinted at and left to the reader\\'s imagination in the book is dwelt upon with complaisance in the TV adaptation: making some scenes both gross and comic, (like when Huntingdon\\'s eye starts bleeding) and others far too sexed up for a period drama! I mean, don\\'t we get enough of those bed scenes in contemporary dramas?')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqmN_Mc-lClh"
      },
      "source": [
        "**Tokenization and Vectorization**\n",
        "* The next step is to perform the Tokenization and Vectorization of the Dataset. I will use Google news pretrained Model Vectors for the process of Vectorization. The Google News Word2vec Vocabulary includes some stopwords as well. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB3wv7UfvtkF",
        "outputId": "2be1ccb2-fed3-4ad5-b917-6e21fa422cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#@ Tokenization and Vectorization:\n",
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"                # Pretrained Word2vec Model.    \n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format(\"/content/GoogleNews-vectors-negative300.bin.gz\",           # Word2vec Model Vectors.\n",
        "                                       binary=True, limit=100000)\n",
        "\n",
        "#@ Function for Tokenization and Vectorization:\n",
        "def tokenize_and_vectorize(dataset):\n",
        "  tokenizer = TreebankWordTokenizer()                                  # Instantiating the Tokenizer.\n",
        "  vectorized_data = []\n",
        "  for sample in dataset:\n",
        "    tokens = tokenizer.tokenize(sample[1])                             # Process for Tokenization.\n",
        "    sample_vecs = []\n",
        "    for token in tokens:\n",
        "      try:\n",
        "        sample_vecs.append(word_vectors[token])                        # Process for Vectorization.\n",
        "      except KeyError:\n",
        "        pass\n",
        "    vectorized_data.append(sample_vecs)\n",
        "  \n",
        "  return vectorized_data                                               # Returning the Vectorized Data.\n",
        "\n",
        "#@ Function for Collecting the Target Labels:\n",
        "def collect_expected(dataset):\n",
        "  \"\"\" Collecting the Target Labels: 0 for Negative Review and 1 for Positive Review. \"\"\"\n",
        "  expected=[]\n",
        "  for sample in dataset:\n",
        "    expected.append(sample[0])\n",
        "  return expected\n",
        "\n",
        "#@ Tokenization and Vectorization:\n",
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJiAM9SZzx7z"
      },
      "source": [
        "**Splitting into Training and Testing.**\n",
        "* Now, I will split the above obtained Dataset into Training set and a Test set. I will split the Dataset into 80% for Training and 20% for Test set. The next code will bucket the Data into Training set X train along with correct labels y train and similarly into Test set X test along with correct labels y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmtgb2HysvRD"
      },
      "source": [
        "#@ Splitting the Dataset into Training set and Test set:\n",
        "split_part = int(len(vectorized_data) * 0.8)\n",
        "\n",
        "#@ Training set:\n",
        "X_train = vectorized_data[:split_part]\n",
        "y_train = expected[:split_part]\n",
        "\n",
        "#@ Test set:\n",
        "X_test = vectorized_data[split_part:]\n",
        "y_test = expected[split_part:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vI19UXrxD6m"
      },
      "source": [
        "### **Convolutional Neural Networks**\n",
        "* In Deep Learning, a Convolutional Neural Network is a class of Deep Neural Networks, most commonly applied to analyzing Visual Imagery. They are also known as shift invariant or space invariant Artificial Neural Networks, based on their shared-weights architecture and translation invariance characteristics. The next blocks of code sets most of the hyperparameters for Convolutional Neural Network.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-NOjr3baseE"
      },
      "source": [
        "#@ Parameters of Convolutional Neural Networks:\n",
        "maxlen = 500                                    # Maximum review length.\n",
        "batch_size = 32                                 # Number of samples shown to the network before updating the weights.\n",
        "embedding_dims = 300                            # Length of token vectors for passing in Convnet.\n",
        "filters = 250                                   # Number of filters required for training.\n",
        "kernel_size = 3                                 # Width of Filters.\n",
        "hidden_dims = 250                               # Number of neurons in feed forward net.\n",
        "epochs = 10                                     # Number of times for passing the training dataset."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zVh25ac0TKj"
      },
      "source": [
        "**Padding and Truncating the Sequence**\n",
        "* **Keras** has the preprocessing helper method called pad_sequences which is used to pad the input Data. But it works only on the sequence of scalars and sequence of vectors. Now, I will write the helper function to pad the input Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-T4CGbV2WEF",
        "outputId": "fc9c8bfb-b971-4a0f-e9fb-0b9dc95c8b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#@ Padding and Truncating the Token Sequence:\n",
        "\n",
        "def pad_trunc(data, maxlen):\n",
        "  \"\"\" Padding the Dataset with zero Vectors. \"\"\"\n",
        "  new_data = []\n",
        "  # Creating zeros vectors of length of Word vectors.\n",
        "  zero_vector = []\n",
        "  for _ in range(len(data[0][0])):\n",
        "    zero_vector.append(0.0)\n",
        "\n",
        "  for sample in data:\n",
        "    if len(sample) > maxlen:\n",
        "      temp = sample[:maxlen]\n",
        "    elif len(sample) < maxlen:\n",
        "      temp = sample \n",
        "      # Append the appropriate number of 0 vectors to the list.\n",
        "      additional_elems = maxlen - len(sample)\n",
        "      for _ in range(additional_elems):\n",
        "        temp.append(zero_vector)\n",
        "    else:\n",
        "      temp = sample \n",
        "    new_data.append(temp)\n",
        "  return new_data\n",
        "\n",
        "\n",
        "#@ Gathering the Truncated and Augmented Data:\n",
        "X_train = pad_trunc(X_train, maxlen)\n",
        "X_test = pad_trunc(X_test, maxlen)\n",
        "\n",
        "#@ Converting the Data into Numpy Arrays:\n",
        "X_train = np.reshape(X_train, (len(X_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.reshape(X_test, (len(X_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "#@ Inspecting the shape of the Data:\n",
        "display(f\"Shape of Training Data {X_train.shape, y_train.shape}\")\n",
        "display(f\"Shape of Testing Data {X_test.shape, y_test.shape}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Shape of Training Data ((1601, 500, 300), (1601,))'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Shape of Testing Data ((401, 500, 300), (401,))'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQO7b4vW97bL"
      },
      "source": [
        "**Convolutional Neural Network**\n",
        "* Now, The Data is ready to build the Neural Network. Each stride in the Convolution will be of one token. And I will be using the ReLU activation Function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvx5wjHo7qsu",
        "outputId": "00f1bdb8-440e-45f0-8de2-b03eced2a73a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "#@ Convolutional Neural Network:\n",
        "model = Sequential()                                           # Standard Model definition for Keras.\n",
        "model.add(Conv1D(                                              # Adding one Convolutional layer.\n",
        "    filters, kernel_size, \n",
        "    padding=\"valid\", activation=\"relu\",\n",
        "    strides=1,\n",
        "    input_shape=(maxlen, embedding_dims)\n",
        "))\n",
        "model.add(GlobalMaxPooling1D())                                # Adding the Pooling layer.\n",
        "model.add(Dense(hidden_dims))                                  # Fully connected Hidden layer.\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation(\"relu\"))                                  # Adding the ReLU Activation layer.\n",
        "model.add(Dense(1))                                            \n",
        "model.add(Activation(\"sigmoid\"))                               # Adding the Sigmoid Activation layer for the Ouptut.\n",
        "\n",
        "#@ Compiling the Convolutional Neural Network:\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#@ Training the Convolutional Neural Network:\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=batch_size,                                     # Number of Data samples processed before backpropagation.\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "#@ Inspecting the Summary of the Model:\n",
        "print(\"\\n\")\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "51/51 [==============================] - 1s 23ms/step - loss: 0.6555 - accuracy: 0.6096 - val_loss: 0.5676 - val_accuracy: 0.7756\n",
            "Epoch 2/10\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.4072 - accuracy: 0.8345 - val_loss: 0.3816 - val_accuracy: 0.8379\n",
            "Epoch 3/10\n",
            "51/51 [==============================] - 1s 17ms/step - loss: 0.2714 - accuracy: 0.8901 - val_loss: 0.3661 - val_accuracy: 0.8404\n",
            "Epoch 4/10\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.1281 - accuracy: 0.9656 - val_loss: 0.3519 - val_accuracy: 0.8479\n",
            "Epoch 5/10\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.0458 - accuracy: 0.9994 - val_loss: 0.3598 - val_accuracy: 0.8354\n",
            "Epoch 6/10\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.0282 - accuracy: 0.9994 - val_loss: 0.3771 - val_accuracy: 0.8529\n",
            "Epoch 7/10\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.3634 - val_accuracy: 0.8529\n",
            "Epoch 8/10\n",
            "51/51 [==============================] - 1s 17ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.3720 - val_accuracy: 0.8579\n",
            "Epoch 9/10\n",
            "51/51 [==============================] - 1s 17ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.3717 - val_accuracy: 0.8579\n",
            "Epoch 10/10\n",
            "51/51 [==============================] - 1s 16ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3766 - val_accuracy: 0.8554\n",
            "\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 498, 250)          225250    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 250)               62750     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 251       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 288,251\n",
            "Trainable params: 288,251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r41yGdheJ2vI"
      },
      "source": [
        "**Saving the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53Du5pIb-5Nz"
      },
      "source": [
        "#@ Saving the Model:\n",
        "model_structure = model.to_json()                            # Saving the structure of the Network.\n",
        "with open(\"cnn_model.json\", \"w\") as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights(\"cnn_weights.h5\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqMMl0J8Ncsx"
      },
      "source": [
        "**Loading the Saved Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssP1Cm9tNbwO"
      },
      "source": [
        "#@ Loading the Saved Model:\n",
        "from keras.models import model_from_json\n",
        "with open(\"cnn_model.json\", \"r\") as json_file:\n",
        "  json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "\n",
        "model.load_weights(\"cnn_weights.h5\")                         # Loading the saved Model.       "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzx4lKKpOrCL"
      },
      "source": [
        "**Model Evaluation**\n",
        "* Now, I will make a sentence with a Positive sentiment. And I will predict the sentiment with the help of Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mid2mBOTOO8o",
        "outputId": "7e62ba76-f490-4427-9b76-22228928078a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@ Model Evaluation:\n",
        "sample_1 = \"Natural Language Processing is one of the most interesting topics in Machine Learning. Many people loves to learn Natural \\\n",
        "            Language Processing in the modern days. Surprisingly, some people doen't like Natural Langugae Processing a lot! I can't wait \\\n",
        "            to learn NLP in future days. I am fond of reading NLP.\"\n",
        "\n",
        "#@ Making Predictions:\n",
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "#@ Inspecting the Prediction:\n",
        "f\"The predicted sentiment by the Model is: {model.predict_classes(test_vec)}\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The predicted sentiment by the Model is: [[1]]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}