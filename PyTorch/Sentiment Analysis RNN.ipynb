{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis: RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbC5ngWVan7F"
      },
      "source": [
        "### **INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_kBioPpZ8fr"
      },
      "source": [
        "#@ INITIALIZATION:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6teUobWau8U"
      },
      "source": [
        "**DOWNLOADING LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7XNpWvtar3d"
      },
      "source": [
        "#@ DOWNLOADING THE LIBRARIES AND DEPENDENCIES:\n",
        "# !pip install -U d2l\n",
        "from d2l import torch as d2l\n",
        "\n",
        "import os\n",
        "import torch     \n",
        "from torch import nn                                \n",
        "from IPython import display"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntcPKrcfbCMo"
      },
      "source": [
        "**GETTING THE DATASET:**\n",
        "- I have used google colab for this project so the process of downloading and reading the data might be different in other platforms. I will use [**Large Movie Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/) for this project. The dataset is divided into training and testing and each contains 25000 movie reviews. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOXM5IY_a0Fe"
      },
      "source": [
        "#@ GETTING THE DATASET: \n",
        "batch_size = 64                                                     # Initializing Batch Size. \n",
        "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)       # Initializing Training and Test Iterations. "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sIUWzcqb2L1"
      },
      "source": [
        "### **RECURRENT NEURAL NETWORK MODEL:**\n",
        "- Each words obtains a feature vector from the embedding layer which is further encoded using bidirectional RNN to obtain sequence information. Here the `Embedding` instance is the embedding layer, the `LSTM` instance is the hidden layer for sequence encoding and the `Dense` instance is the output layer for generated classification result. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diqedsHmbntm"
      },
      "source": [
        "#@ INITIALIZING RECURRENT NEURAL NETWORK MODEL: \n",
        "class BiRNN(nn.Module):                                                       # Initializing Bidirectional RNN. \n",
        "  def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, \n",
        "               **kwargs):                                                     # Initializing Constructor Function. \n",
        "    super(BiRNN, self).__init__(**kwargs)\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size)                     # Initializing Embedding Layer. \n",
        "    self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, \n",
        "                           bidirectional=True)                                # Initializing Bidirectional LSTM. \n",
        "    self.decoder = nn.Linear(4*num_hiddens, 2)                                # Initializing Linear Layer. \n",
        "  \n",
        "  def forward(self, inputs):                                                  # Forward Propagation Function. \n",
        "    embeddings = self.embedding(inputs.T)                                     # Implementation of Embedding Layer. \n",
        "    self.encoder.flatten_parameters()    \n",
        "    outputs, _ = self.encoder(embeddings)                                     # Implementation of LSTM. \n",
        "    encoding = torch.cat((outputs[0], outputs[-1]), dim=1)                    # Concatenating Initial and Final Timestep. \n",
        "    outs = self.decoder(encoding)                                             # Implementation of Linear Layer. \n",
        "    return outs"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiYXli7hho91"
      },
      "source": [
        "#@ IMPLEMENTATION OF RECURRENT NEURAL NETWORKS: \n",
        "embed_size, num_hiddens = 100, 100                              # Initialization of Parameters. \n",
        "num_layers, devices = 2, d2l.try_all_gpus()                     # Initialization of Parameters. \n",
        "net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)    # Initialization of Bidirectional RNN Model. \n",
        "\n",
        "#@ INITIALIZATION OF WEIGHTS: \n",
        "def init_weights(m):                                            # Function for Initializing Weights. \n",
        "  if type(m) == nn.Linear:\n",
        "    nn.init.xavier_uniform_(m.weight)                           # Xavier Initialization. \n",
        "  if type(m) == nn.LSTM:\n",
        "    for param in m._flat_weights_names:\n",
        "      if \"weight\" in param:\n",
        "        nn.init.xavier_uniform_(m._parameters[param])           # Xavier Initialization. \n",
        "net.apply(init_weights);                                        # Initializing Weights. "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN5t9HO2j5qg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}