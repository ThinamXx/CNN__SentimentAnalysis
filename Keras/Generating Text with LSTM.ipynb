{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating Text with LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwQgcbi_TkSn"
      },
      "source": [
        "**Initialization**\n",
        "* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a Project or Problem. And the third line of code helps to make visualization within the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzd-hKDmTHgH"
      },
      "source": [
        "#@ Initialization:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haBU6S5gWf27"
      },
      "source": [
        "**Downloading the Dependencies**\n",
        "* I have downloaded all the Libraries and Dependencies required for this Project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfcP2_GrWJfS"
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies:\n",
        "import nltk                                             # Python Library for NLP.\n",
        "from nltk.corpus import gutenberg                       # Text Corpus.\n",
        "\n",
        "import os, glob, random\n",
        "from random import shuffle\n",
        "from IPython.display import display\n",
        "\n",
        "import numpy as np                                      # Module to work with Arrays.\n",
        "from keras.preprocessing import sequence                # Module to handle Padding Input.\n",
        "from keras.models import Sequential                     # Base Keras Neural Network Model.\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dense, Dropout, Flatten        # Layers Objects to pile into Model.\n",
        "from keras.layers import LSTM, GRU                      # Convolutional Layer and MaxPooling.\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer         # Module for Tokenization.\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkzHF5olYjPh"
      },
      "source": [
        "**Getting the Data**\n",
        "* I need a Dataset which is more consistent across samples in style and tone or a much larger Dataset. The Keras Example provides a sample of the work of Friedrich Nietzsche. But I will choose someone else with a singular style : William Shakespeare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5t9W8AUYY8t",
        "outputId": "0a4795a3-6168-4dcd-8219-e5b0a691478d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#@ Getting the Data:\n",
        "nltk.download(\"gutenberg\")                               # Downloading the Text Corpus.\n",
        "\n",
        "#@ Inspecting the Data:\n",
        "display(gutenberg.fileids())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hNvliEUdGIC"
      },
      "source": [
        "**Processing the Shakespeare Plays**\n",
        "* There are 3 plays of William Shakespeare in Gutenberg Corpus as shown above. Now, I will concatenate all Shakespeare plays in Gutenberg Corpus into a large string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEnl8sUjc29E",
        "outputId": "68cc704b-a366-4b13-caef-68e07d62d244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#@ Processing the Shakespeare Plays:\n",
        "text = \" \"\n",
        "for txt in gutenberg.fileids():\n",
        "  if \"shakespeare\" in txt:\n",
        "    text += gutenberg.raw(txt).lower()\n",
        "\n",
        "chars = sorted(list(set(text)))       \n",
        "char_indices = dict((c, i) for i,c in enumerate(chars))             # Making a dictionary of characters to an index.\n",
        "indices_char = dict((i, c) for i,c in enumerate(chars))             # Making a opposite dictionary of characters to an index.\n",
        "\n",
        "#@ Inspecting the Corpus and Characters:\n",
        "display(f\"Corpus Length: {len(text)}\")\n",
        "display(f\"Total Characters: {len(chars)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Corpus Length: 375543'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Total Characters: 50'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS2w1Lo-iENC",
        "outputId": "b4d76142-fd44-40e9-a635-8efad5611807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "#@ Inspecting the Formatting of Text:\n",
        "print(text[:500])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " [the tragedie of julius caesar by william shakespeare 1599]\n",
            "\n",
            "\n",
            "actus primus. scoena prima.\n",
            "\n",
            "enter flauius, murellus, and certaine commoners ouer the stage.\n",
            "\n",
            "  flauius. hence: home you idle creatures, get you home:\n",
            "is this a holiday? what, know you not\n",
            "(being mechanicall) you ought not walke\n",
            "vpon a labouring day, without the signe\n",
            "of your profession? speake, what trade art thou?\n",
            "  car. why sir, a carpenter\n",
            "\n",
            "   mur. where is thy leather apron, and thy rule?\n",
            "what dost thou with thy best apparrell o\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuSBrlMjiyde"
      },
      "source": [
        "**Assembling the Training Set**\n",
        "* Now, I will chop up the source text into each Data samples with fixed set of characters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7WYPnPWidzz",
        "outputId": "11dede23-3967-4ceb-dd5d-3d0404b790d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@ Assembling the Training Set:\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):                     # Step by 3 characters so the samples will overlap.\n",
        "  sentences.append(text[i:i + maxlen])                           # Grab a slice of the text.\n",
        "  next_chars.append(text[i + maxlen])                            # Collecting the next expected character.\n",
        "\n",
        "#@ Inspecting the Sequences:\n",
        "display(f\"Sequences: {len(sentences)}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Sequences: 125168'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFj1aKGtlCW3"
      },
      "source": [
        "#@ Onehot Encoding the Training Examples:\n",
        "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, char in enumerate(sentence):\n",
        "    X[i, t, char_indices[char]] = 1\n",
        "  y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKbPF5DRnTHp"
      },
      "source": [
        "**Long Short Term Memory**\n",
        "* Long Short Term Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of Deep Learning. Unlike standard Feedforward Neural Networks, LSTM has Feedback connections. It can not only process single data points, but also entire sequences of data such as Speech or Video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX8Jl-g7nNMQ",
        "outputId": "4d1c511d-43af-4585-e1ba-819f8f1ea988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "#@ Long Short Term Memory or LSTM:\n",
        "maxlen = 40\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "model = Sequential()                               # Standard Model Definition for Keras.\n",
        "model.add(LSTM(                                    # Adding the LSTM Layer.\n",
        "    units=128, \n",
        "    input_shape=(maxlen, len(chars))\n",
        "))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "#@ Compiling the LSTM Neural Network:\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=RMSprop(learning_rate=0.01),\n",
        ")\n",
        "\n",
        "#@ Training the LSTM Model:\n",
        "model.fit(\n",
        "    X, y,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "#@ Inspecting the Summary of the Model:\n",
        "print(\"\\n\")\n",
        "model.summary()                                           # Summary of the Model."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "978/978 [==============================] - 5s 5ms/step - loss: 2.0558\n",
            "Epoch 2/10\n",
            "978/978 [==============================] - 5s 5ms/step - loss: 1.6958\n",
            "Epoch 3/10\n",
            "978/978 [==============================] - 5s 5ms/step - loss: 1.5899\n",
            "Epoch 4/10\n",
            "978/978 [==============================] - 5s 5ms/step - loss: 1.5289\n",
            "Epoch 5/10\n",
            "978/978 [==============================] - 5s 5ms/step - loss: 1.4873\n",
            "Epoch 6/10\n",
            "978/978 [==============================] - 5s 5ms/step - loss: 1.4529\n",
            "Epoch 7/10\n",
            "978/978 [==============================] - 5s 5ms/step - loss: 1.4291\n",
            "Epoch 8/10\n",
            "978/978 [==============================] - 5s 5ms/step - loss: 1.4106\n",
            "Epoch 9/10\n",
            "978/978 [==============================] - 5s 5ms/step - loss: 1.3926\n",
            "Epoch 10/10\n",
            "978/978 [==============================] - 5s 5ms/step - loss: 1.3778\n",
            "\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               91648     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 50)                6450      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 50)                0         \n",
            "=================================================================\n",
            "Total params: 98,098\n",
            "Trainable params: 98,098\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouZJUKMsDRt"
      },
      "source": [
        "* The output vectors are 50 D Vectors describing a probability distribution over the 50 possible output characters so that I can sample the distribution. The **Keras** example has the helper function just to do that as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTt9hu60rTw9"
      },
      "source": [
        "#@ Sampler to generate character sequences:\n",
        "def sample(preds, temperature=1.0):\n",
        "  preds = np.asarray(preds).astype(\"float64\")\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sFWA8couo_L"
      },
      "source": [
        "**Generating the Text with Diversity Level**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O7jq4CXul3Q",
        "outputId": "e10a885b-7364-4596-bdf1-9945d652a6bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "#@ Generating the Text with Diversity Level:\n",
        "import sys\n",
        "start_index = random.randint(0, len(text)-maxlen-1)\n",
        "for diversity in [0.2, 0.5, 1.0]:\n",
        "  print()\n",
        "  print(\"---- Diversity:\", diversity)\n",
        "  generated = \" \"\n",
        "  sentence = text[start_index: start_index + maxlen]\n",
        "  generated += sentence\n",
        "  print(\"---- Generating with seed:\")\n",
        "  sys.stdout.write(generated)\n",
        "  for i in range(400):\n",
        "    x = np.zeros((1, maxlen, len(chars)))\n",
        "    for t, char in enumerate(sentence):\n",
        "      x[0, t, char_indices[char]] = 1.\n",
        "    preds = model.predict(x, verbose=0)[0]                             # Making prediction with Model.\n",
        "    next_index = sample(preds, diversity)          \n",
        "    next_char = indices_char[next_index]                             # Looking the character that index represents.\n",
        "    generated += next_char \n",
        "    sentence = sentence[1:] + next_char \n",
        "    sys.stdout.write(next_char) \n",
        "    sys.stdout.flush()                                                # Flushes the internal buffer.\n",
        "  print()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- Diversity: 0.2\n",
            "---- Generating with seed:\n",
            " e we were two dayes old at sea, a pyrate\n",
            "\n",
            "   hor. i haue thee the sended the did so man,\n",
            "the sent the sent in the sent the sent soules\n",
            "\n",
            "   ham. i haue thee the starres in the seene of their part,\n",
            "and what i say the senions sir, and stand,\n",
            "and there is a true the status soules;\n",
            "if the sent shall stay thee stand of the saue\n",
            "i would stay to the starres in the senie,\n",
            "and what we haue the sent in the word the send\n",
            "\n",
            "   ham. all the sent to th\n",
            "\n",
            "---- Diversity: 0.5\n",
            "---- Generating with seed:\n",
            " e we were two dayes old at sea, a pyrate\n",
            "\n",
            "   ophe. how heard him a thing of this region be one\n",
            "the seene not something of the boyse shall doe\n",
            "to his postion to be thee the senion in thee thee sute\n",
            "\n",
            "   mes. the honor in the say'n, heere of great a too\n",
            "\n",
            "   hor. i am must it heares it did must thee,\n",
            "and there is not the starres in my selfe;\n",
            "and heere in the put vp not will and be friends\n",
            "\n",
            "   ham. friends to the person, and their parthance\n",
            "\n",
            "\n",
            "---- Diversity: 1.0\n",
            "---- Generating with seed:\n",
            " e we were two dayes old at sea, a pyrate\n",
            "\n",
            "   nies, and com'ling if vrroofe walk pray\n",
            "eye then had fares the will:\n",
            "and still shell pyrruse, so alrest of my chaige the time,\n",
            "as geane demelly, marty seeres, whope. for she stop\n",
            "my life breedon, ere and hamlet, and lall by wine la:\n",
            "i ?\n",
            "  hord. brutus goe, new speak prowhere to his finst;\n",
            "it must condridion of orinanty calch man:\n",
            "to are shand thriuion in hor horce,\n",
            "and word the faineue word a\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}