{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent Neural Network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nkAeFrxXoD-"
      },
      "source": [
        "**Initialization**\n",
        "* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a Project or Problem. And the third line of code helps to make visualization within the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiMnWL_8TtJY"
      },
      "source": [
        "#@ Initialization:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2 \n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baSu49AeX1_K"
      },
      "source": [
        "**Downloading the Dependencies**\n",
        "* I have downloaded all the Libraries and Dependencies required for this Project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br13qwefURns"
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies:\n",
        "import os, glob\n",
        "from random import shuffle\n",
        "from IPython.display import display\n",
        "\n",
        "import numpy as np                                      # Module to work with Arrays.\n",
        "from keras.preprocessing import sequence                # Module to handle Padding Input.\n",
        "from keras.models import Sequential                     # Base Keras Neural Network Model.\n",
        "from keras.layers import Dense, Dropout, Flatten        # Layers Objects to pile into Model.\n",
        "from keras.layers import SimpleRNN                      # Convolutional Layer and MaxPooling.\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer         # Module for Tokenization.\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBYBo1e0ZjfO"
      },
      "source": [
        "**Getting the Data**\n",
        "* I have used Google Colab for this Project so the process of downloading and reading the Data might be different in other platforms. I have used [**Large Moview Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/) for this Project. This is a dataset for binary sentiment classification containing substantially more data. The Dataset has a set of 25,000 highly polar movie reviews for training and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiJVaoq4Ulsc"
      },
      "source": [
        "#@ Getting the Data:\n",
        "def preprocess_data(filepath):\n",
        "  positive_path = os.path.join(filepath, \"pos\")\n",
        "  negative_path = os.path.join(filepath, \"neg\")\n",
        "  pos_label = 1\n",
        "  neg_label = 0\n",
        "  dataset = []\n",
        "  \n",
        "  for filename in glob.glob(os.path.join(positive_path, '*.txt')):                            # Positive Sentiment Dataset.\n",
        "    with open(filename, \"r\") as f:\n",
        "      dataset.append((pos_label, f.read()))\n",
        "  for filename in glob.glob(os.path.join(negative_path, '*.txt')):                            # Negative Sentiment Dataset.\n",
        "    with open(filename, \"r\") as f:\n",
        "      dataset.append((neg_label, f.read()))\n",
        "\n",
        "  shuffle(dataset)                                                                            # Shuffling the Dataset.\n",
        "  return dataset "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WJza2sEZ8pj"
      },
      "source": [
        "**Processing the Dataset**\n",
        "* I have manually downloaded the Dataset from [**Large Moview Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/). I have used the small subset of Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFq5g7zHUpp_",
        "outputId": "f385939d-a67d-4943-971a-a311d3047fd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#@ Processing the Dataset:\n",
        "PATH = \"/content/drive/My Drive/Colab Notebooks/Data/Smalltrain\"                     # Path to the Dataset.\n",
        "dataset = preprocess_data(PATH)                                                      # Processing the Dataset.\n",
        "\n",
        "#@ Inspecting the Dataset:\n",
        "dataset[:3]                                                                          # Inspecting the Dataset."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1,\n",
              "  \"For me too, this Christmas special is one that I remember very fondly. In 1989, I snatched up the 2 CDs I found of the soundtrack recording, giving one to my sister and keeping the other for myself. It's part of my family's Christmas tradition now, and I would love to be able to actually see the show again rather than just remember it as I listen.<br /><br />It has been noted elsewhere that John Denver made a number of appearances on the Muppet Show, and they did more than one special together. The good rapport between Denver and his fuzzy companions comes through clearly here, in a charming and fun show that is good for all ages.\"),\n",
              " (0,\n",
              "  \"Live! Yes, but not kicking.<br /><br />True story: Some time ago, a Dutch TV station made an announcement that they were going to air a new reality show. A contest rather. The main participant in this show would be a woman who was dying of something terrible and she would be donating her kidneys to one lucky person with progressive kidney failure. For real.<br /><br />The country and the international media were all over this story like flies on a turd, saying it was appalling, immoral, what-is-this-world-coming-to, and the like. In a way, I had to agree.<br /><br />As the months passed, the tension built up to a degree that the government was mostly occupied by the issue of whether they should let this show go ahead or not, instead of running the country.<br /><br />The show did air and right up to the last moment they were pushing ahead. And up to the last moment the country was up in arms, the Prime Minister making speeches, every newspaper writing about it, everyone in the country holding their breaths. And the network pushed on. Towards a new frontier in television. And they definitely succeeded in doing just that. They pushed the envelope.<br /><br />The show aired and we all watched a terminally ill woman selecting the right candidate to receive her kidneys so he or she would live, whilst she would die shortly after.<br /><br />And then, in the last moments of the show it was revealed that it was a partial hoax. The woman was not ill, but all the candidates were. There was no kidney auction. The whole show, that, with the publicity and the commercials and all the discussions, built up for months to a fantastic climax, was a publicity stunt to focus attention on the problem of major shortages in organ donors. The man who founded this particular network himself died of kidney disease.<br /><br />Now THIS is television. Leaving everybody far behind in amazement.<br /><br />Don't give me a poorly acted, poorly directed flick about some woman trying to get a Russian Roulette show on American TV.<br /><br />As if.<br /><br />*Spoiler* As if I'm going to believe they would get this through the FCC. As if I'm going to believe this would get through the US Supreme Court on the basis of free expression. As if I'm gonna believe the ridiculous ending where this woman pulled it off and has conscience issues because some guy shot himself on air.<br /><br />It's all been done before. Watch Running Man with Arnold instead. At least it had a semi good ending.<br /><br />*Spoiler* This is an appallingly bad piece of film, together with a ridiculous ending. So she gets shot in the end, is that supposed to make us movie going public feel better after we leave the theater because there was some kind of justice? Don't take my word for it, but I would say this: leave this one alone and watch a test pattern instead, you'll get more quality.\"),\n",
              " (0,\n",
              "  'Sloppily directed, witless comedy that supposedly spoofs the \"classic\" 50s \"alien invasion\" films, but really is no better than them, except of course in the purely technical department (good makeup effects). And any spoof that is worse than its target is doomed to fail (\"Casino Royale\", \"Our Man Flint\" are worse than almost any James Bond movie). After two hours of hearing the screeching voices of the aliens, you\\'ll be begging for some peace and quiet. (*1/2)')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPbDg0vdaJlD"
      },
      "source": [
        "**Tokenization and Vectorization**\n",
        "* The next step is to perform the Tokenization and Vectorization of the Dataset. I will use Google news pretrained Model Vectors for the process of Vectorization. The Google News Word2vec Vocabulary includes some stopwords as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvIm8m6dU45d",
        "outputId": "94d34f8c-ac50-4ebe-bc47-a42621d90a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#@ Tokenization and Vectorization:\n",
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"                # Pretrained Word2vec Model.    \n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format(\"/content/GoogleNews-vectors-negative300.bin.gz\",           # Word2vec Model Vectors.\n",
        "                                       binary=True, limit=100000)\n",
        "\n",
        "#@ Function for Tokenization and Vectorization:\n",
        "def tokenize_and_vectorize(dataset):\n",
        "  tokenizer = TreebankWordTokenizer()                                  # Instantiating the Tokenizer.\n",
        "  vectorized_data = []\n",
        "  for sample in dataset:\n",
        "    tokens = tokenizer.tokenize(sample[1])                             # Process for Tokenization.\n",
        "    sample_vecs = []\n",
        "    for token in tokens:\n",
        "      try:\n",
        "        sample_vecs.append(word_vectors[token])                        # Process for Vectorization.\n",
        "      except KeyError:\n",
        "        pass\n",
        "    vectorized_data.append(sample_vecs)\n",
        "  \n",
        "  return vectorized_data                                               # Returning the Vectorized Data.\n",
        "\n",
        "#@ Function for Collecting the Target Labels:\n",
        "def collect_expected(dataset):\n",
        "  \"\"\" Collecting the Target Labels: 0 for Negative Review and 1 for Positive Review. \"\"\"\n",
        "  expected=[]\n",
        "  for sample in dataset:\n",
        "    expected.append(sample[0])\n",
        "  return expected\n",
        "\n",
        "#@ Tokenization and Vectorization:\n",
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiDSccuka6yO"
      },
      "source": [
        "**Splitting into Training and Testing.**\n",
        "* Now, I will split the above obtained Dataset into Training set and a Test set. I will split the Dataset into 80% for Training and 20% for Test set. The next code will bucket the Data into Training set X train along with correct labels y train and similarly into Test set X test along with correct labels y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZRiVHNqajZS"
      },
      "source": [
        "#@ Splitting the Dataset into Training set and Test set:\n",
        "split_part = int(len(vectorized_data) * 0.8)\n",
        "\n",
        "#@ Training set:\n",
        "X_train = vectorized_data[:split_part]\n",
        "y_train = expected[:split_part]\n",
        "\n",
        "#@ Test set:\n",
        "X_test = vectorized_data[split_part:]\n",
        "y_test = expected[split_part:]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6FPCEtsbcke"
      },
      "source": [
        "### **Recurrent Neural Networks**\n",
        "* A Recurrent Neural Network is a class of Artificial Neural Networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal Dynamic behavior. Derived from Feedforward Neural Networks, RNN can use their internal state or memory to process variable length sequences of inputs. This makes them applicable to tasks such as Unsegmented and Connected Handwriting Recognition or Speech Recognition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2-QGTPobKl1"
      },
      "source": [
        "#@ Parameters of Recurrent Neural Networks:\n",
        "maxlen = 500                                    # Maximum review length.\n",
        "batch_size = 32                                 # Number of samples shown to the network before updating the weights.\n",
        "embedding_dims = 300                            # Length of token vectors for passing in RNN.\n",
        "epochs = 10                                     # Number of times for passing the training dataset.\n",
        "num_neurons = 50"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm7WKvf-c-nW"
      },
      "source": [
        "**Padding and Truncating the Sequence**\n",
        "* **Keras** has the preprocessing helper method called pad_sequences which is used to pad the input Data. But it works only on the sequence of scalars and sequence of vectors. Now, I will write the helper function to pad the input Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akXV4BUSc58l",
        "outputId": "ff82625f-ee55-4032-c4a0-d500995c02d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#@ Padding and Truncating the Token Sequence:\n",
        "\n",
        "def pad_trunc(data, maxlen):\n",
        "  \"\"\" Padding the Dataset with zero Vectors. \"\"\"\n",
        "  new_data = []\n",
        "  # Creating zeros vectors of length of Word vectors.\n",
        "  zero_vector = []\n",
        "  for _ in range(len(data[0][0])):\n",
        "    zero_vector.append(0.0)\n",
        "\n",
        "  for sample in data:\n",
        "    if len(sample) > maxlen:\n",
        "      temp = sample[:maxlen]\n",
        "    elif len(sample) < maxlen:\n",
        "      temp = sample \n",
        "      # Append the appropriate number of 0 vectors to the list.\n",
        "      additional_elems = maxlen - len(sample)\n",
        "      for _ in range(additional_elems):\n",
        "        temp.append(zero_vector)\n",
        "    else:\n",
        "      temp = sample \n",
        "    new_data.append(temp)\n",
        "  return new_data\n",
        "\n",
        "\n",
        "#@ Gathering the Truncated and Augmented Data:\n",
        "X_train = pad_trunc(X_train, maxlen)\n",
        "X_test = pad_trunc(X_test, maxlen)\n",
        "\n",
        "#@ Converting the Data into Numpy Arrays:\n",
        "X_train = np.reshape(X_train, (len(X_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.reshape(X_test, (len(X_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "#@ Inspecting the shape of the Data:\n",
        "display(f\"Shape of Training Data {X_train.shape, y_train.shape}\")\n",
        "display(f\"Shape of Testing Data {X_test.shape, y_test.shape}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Shape of Training Data ((1601, 500, 300), (1601,))'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Shape of Testing Data ((401, 500, 300), (401,))'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNzDrEnfd8EO"
      },
      "source": [
        "**Recurrent Neural Network**\n",
        "* Now, The Dataset is ready to build the Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvoYurE8dGcL",
        "outputId": "001819ea-4341-45c6-c0c4-e92bcac2dfc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "#@ Simple Recurrent Neural Network:\n",
        "model = Sequential()                                         # Standard Model definition for Keras.\n",
        "model.add(SimpleRNN(                                         # Adding the Recurrent layer.\n",
        "    num_neurons, return_sequences=True,\n",
        "    input_shape=(maxlen, embedding_dims)\n",
        "))\n",
        "model.add(Dropout(0.2))                                      # Adding the Dropout layer.\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation=\"sigmoid\"))                    # Using sigmoid activation.\n",
        "\n",
        "#@ Compiling the Recurrent Neural Network:\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"rmsprop\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#@ Training the Recurrent Neural Network:\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "#@ Inspecting the Summary of the Model:\n",
        "print(\"\\n\")\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "51/51 [==============================] - 14s 281ms/step - loss: 0.8109 - accuracy: 0.5478 - val_loss: 0.7191 - val_accuracy: 0.5860\n",
            "Epoch 2/10\n",
            "51/51 [==============================] - 13s 264ms/step - loss: 0.5038 - accuracy: 0.7533 - val_loss: 1.4836 - val_accuracy: 0.4863\n",
            "Epoch 3/10\n",
            "51/51 [==============================] - 13s 264ms/step - loss: 0.3799 - accuracy: 0.8413 - val_loss: 0.6599 - val_accuracy: 0.6783\n",
            "Epoch 4/10\n",
            "51/51 [==============================] - 14s 271ms/step - loss: 0.2758 - accuracy: 0.8944 - val_loss: 0.7812 - val_accuracy: 0.6633\n",
            "Epoch 5/10\n",
            "51/51 [==============================] - 14s 269ms/step - loss: 0.1950 - accuracy: 0.9375 - val_loss: 0.7439 - val_accuracy: 0.6833\n",
            "Epoch 6/10\n",
            "51/51 [==============================] - 14s 265ms/step - loss: 0.1413 - accuracy: 0.9563 - val_loss: 0.6914 - val_accuracy: 0.7182\n",
            "Epoch 7/10\n",
            "51/51 [==============================] - 13s 265ms/step - loss: 0.0936 - accuracy: 0.9775 - val_loss: 0.8321 - val_accuracy: 0.7082\n",
            "Epoch 8/10\n",
            "51/51 [==============================] - 14s 273ms/step - loss: 0.0612 - accuracy: 0.9881 - val_loss: 0.8197 - val_accuracy: 0.7032\n",
            "Epoch 9/10\n",
            "51/51 [==============================] - 14s 268ms/step - loss: 0.0381 - accuracy: 0.9963 - val_loss: 0.9239 - val_accuracy: 0.7032\n",
            "Epoch 10/10\n",
            "51/51 [==============================] - 13s 263ms/step - loss: 0.0331 - accuracy: 0.9944 - val_loss: 1.1947 - val_accuracy: 0.6683\n",
            "\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 500, 50)           17550     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 500, 50)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 25001     \n",
            "=================================================================\n",
            "Total params: 42,551\n",
            "Trainable params: 42,551\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfCKwV1XojDA"
      },
      "source": [
        "**Building the Larger Network**\n",
        "* I will use 100 neurons instead of 50 in the Model defined below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I136SYxxjyYQ",
        "outputId": "364a8890-c349-4730-80a7-7b302aa47e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "#@ Simple Recurrent Neural Network:\n",
        "num_neurons=100                                              # Adding 100 Neurons.\n",
        "epochs=5\n",
        "model = Sequential()                                         # Standard Model definition for Keras.\n",
        "model.add(SimpleRNN(                                         # Adding the Recurrent layer.\n",
        "    num_neurons, return_sequences=True,\n",
        "    input_shape=(maxlen, embedding_dims)\n",
        "))\n",
        "model.add(Dropout(0.2))                                      # Adding the Dropout layer.\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation=\"sigmoid\"))                    # Using sigmoid activation.\n",
        "\n",
        "#@ Compiling the Recurrent Neural Network:\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"rmsprop\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#@ Training the Recurrent Neural Network:\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "#@ Inspecting the Summary of the Model:\n",
        "print(\"\\n\")\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "51/51 [==============================] - 14s 272ms/step - loss: 1.3031 - accuracy: 0.5484 - val_loss: 1.1808 - val_accuracy: 0.5337\n",
            "Epoch 2/5\n",
            "51/51 [==============================] - 14s 269ms/step - loss: 0.7074 - accuracy: 0.6708 - val_loss: 0.9486 - val_accuracy: 0.5411\n",
            "Epoch 3/5\n",
            "51/51 [==============================] - 14s 268ms/step - loss: 0.5712 - accuracy: 0.7701 - val_loss: 1.0971 - val_accuracy: 0.5636\n",
            "Epoch 4/5\n",
            "51/51 [==============================] - 13s 264ms/step - loss: 0.3470 - accuracy: 0.8545 - val_loss: 1.1425 - val_accuracy: 0.5761\n",
            "Epoch 5/5\n",
            "51/51 [==============================] - 14s 269ms/step - loss: 0.2569 - accuracy: 0.8957 - val_loss: 0.9321 - val_accuracy: 0.6259\n",
            "\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_1 (SimpleRNN)     (None, 500, 100)          40100     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 500, 100)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 50000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 50001     \n",
            "=================================================================\n",
            "Total params: 90,101\n",
            "Trainable params: 90,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDVOxugwp-ic"
      },
      "source": [
        "**Saving the Recurrent Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygof-i99o9md",
        "outputId": "e6e62e22-6881-4fcf-bb6b-dcf1774f52c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@ Saving the Recurrent Neural Network:\n",
        "model_structure = model.to_json()\n",
        "with open(\"simplernn_model.json\", \"w\") as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights(\"simplernn_model.h5\")\n",
        "print(\"Model saved!!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKxPNWRiFAMb"
      },
      "source": [
        "**Model Evaluation**\n",
        "* Now, I have trained a Model. I will make a sentence with Positive Sentiment and I will predict the Sentiment of the sentence using the Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GY7PK_Tqtkb",
        "outputId": "ffece1c7-a759-4b59-979d-c184300d9296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@ Model Evaluation:\n",
        "sample_1 = \"Natural Language Processing is one of the most interesting topics in Machine Learning. Many people loves to learn Natural \\\n",
        "            Language Processing in the modern days. Surprisingly, some people doen't like Natural Langugae Processing a lot! I can't wait \\\n",
        "            to learn NLP in future days. I am fond of reading NLP.\"\n",
        "\n",
        "#@ Making Predictions:\n",
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "#@ Inspecting the Prediction:\n",
        "f\"The predicted sentiment by the Model is: {model.predict_classes(test_vec)}\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The predicted sentiment by the Model is: [[1]]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mszY-2vhLEV9"
      },
      "source": [
        "**Note:**\n",
        "* The Simple Recurrent Neural Network as mentioned above is not the better choice. They are relatively expensive to train and pass new samples through compared to Feedforward net or Convolutional Neural Net. At least the results obtained above are not appreciably better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjxAvfWVNMyb"
      },
      "source": [
        "## **Bidirectional Recurrent Neural Network**\n",
        "* **Keras** added a layer wrapper that will automatically flip around the necessary inputs and outputs to automatically assemble a Bidirectional Recurrent Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySwyKlwDMDvf",
        "outputId": "f1a0771c-bf70-47b7-8d67-0bb421c78675",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "#@ Parameters of Recurrent Neural Network:\n",
        "num_neurons = 20    \n",
        "maxlen = 100                                      # Maximum review length.\n",
        "embedding_dims = 300                              # Length of token vectors for passing in RNN.\n",
        "\n",
        "#@ Bidirectional Recurrent Neural Network:\n",
        "model = Sequential()                              # Standard Model Definition for Keras.\n",
        "model.add(Bidirectional(SimpleRNN(                # Bidirectional Neural Network.\n",
        "    num_neurons, return_sequences=True,\n",
        "    input_shape=(maxlen, embedding_dims)\n",
        ")))\n",
        "model.add(Dropout(0.2))                                      # Adding the Dropout layer.\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation=\"sigmoid\"))                    # Using sigmoid activation.\n",
        "\n",
        "#@ Compiling the Recurrent Neural Network:\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"rmsprop\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#@ Training the Recurrent Neural Network:\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "#@ Inspecting the Summary of the Model:\n",
        "print(\"\\n\")\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "51/51 [==============================] - 26s 503ms/step - loss: 0.7219 - accuracy: 0.5303 - val_loss: 0.8390 - val_accuracy: 0.4913\n",
            "Epoch 2/5\n",
            "51/51 [==============================] - 25s 495ms/step - loss: 0.4897 - accuracy: 0.7839 - val_loss: 0.9307 - val_accuracy: 0.5387\n",
            "Epoch 3/5\n",
            "51/51 [==============================] - 25s 495ms/step - loss: 0.3618 - accuracy: 0.8501 - val_loss: 0.6536 - val_accuracy: 0.6708\n",
            "Epoch 4/5\n",
            "51/51 [==============================] - 25s 497ms/step - loss: 0.2658 - accuracy: 0.9001 - val_loss: 1.0916 - val_accuracy: 0.6060\n",
            "Epoch 5/5\n",
            "51/51 [==============================] - 25s 494ms/step - loss: 0.2078 - accuracy: 0.9313 - val_loss: 0.6273 - val_accuracy: 0.7007\n",
            "\n",
            "\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional (Bidirectional (None, 500, 40)           12840     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 500, 40)           0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 20000)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 20001     \n",
            "=================================================================\n",
            "Total params: 32,841\n",
            "Trainable params: 32,841\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}