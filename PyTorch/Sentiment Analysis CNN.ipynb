{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teE4QXUcbYpq"
      },
      "source": [
        "### **INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT-EOOHfZoz4"
      },
      "source": [
        "#@ INITIALIZATION:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuV7o9BJberp"
      },
      "source": [
        "**DOWNLOADING LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynAaxjuGbcQI"
      },
      "source": [
        "#@ DOWNLOADING THE LIBRARIES AND DEPENDENCIES:\n",
        "# !pip install -U d2l\n",
        "from d2l import torch as d2l\n",
        "\n",
        "import os\n",
        "import torch     \n",
        "from torch import nn                                \n",
        "from IPython import display"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HUE0Dbvcalu"
      },
      "source": [
        "**GETTING THE DATASET:**\n",
        "- I have used google colab for this project so the process of downloading and reading the data might be different in other platforms. I will use [**Large Movie Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/) for this project. The dataset is divided into training and testing and each contains 25000 movie reviews. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSvg7F1lbl0w"
      },
      "source": [
        "#@ GETTING THE DATASET: \n",
        "batch_size = 64                                                     # Initializing Batch Size. \n",
        "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)       # Initializing Training and Test Iterations. "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mKxXw6GheyH"
      },
      "source": [
        "### **TEXT CONVOLUTIONAL NEURAL NETWORKS:**\n",
        "- Text CNN uses a one dimensional convolutional layer and max over time pooling layer. I will use two embedding layers: one with fixed weight and another that participates in training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKRWdeq7cfha"
      },
      "source": [
        "#@ IMPLEMENTATION OF TEXT CONVOLUTIONAL NEURAL NETWORKS: \n",
        "class TextCNN(nn.Module):                                                     # Initializing Text CNN. \n",
        "  def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels, \n",
        "               **kwargs):                                                     # Initializing Constructor Function. \n",
        "    super(TextCNN, self).__init__(**kwargs)\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size)                     # Initializing Embedding Layer. \n",
        "    self.constant_embedding = nn.Embedding(vocab_size, embed_size)            # Initializing Constant Embedding Layer. \n",
        "    self.dropout = nn.Dropout(0.5)                                            # Initializing Dropout Layer. \n",
        "    self.decoder = nn.Linear(sum(num_channels), 2)                            # Initializing Linear Layer. \n",
        "    self.pool = nn.AdaptiveAvgPool1d(1)                                       # Initializing Pooling Layer. \n",
        "    self.relu = nn.ReLU()                                                     # Initializing RELU Activation Function. \n",
        "    self.convs = nn.ModuleList()\n",
        "    for c, k in zip(num_channels, kernel_sizes):\n",
        "      self.convs.append(nn.Conv1d(2 * embed_size, c, k))                      # Initializing Convolutional Layer. \n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    embeddings = torch.cat((self.embedding(inputs), \n",
        "                            self.constant_embedding(inputs)), dim=2)          # Concatenating output of Embedding Layers. \n",
        "    embeddings = embeddings.permute(0, 2, 1)                                  # Changing Dimensions. \n",
        "    encoding = torch.cat([torch.squeeze(\n",
        "        self.relu(self.pool(conv(embeddings))), dim=-1) for \n",
        "        conv in self.convs], dim=1)                                           \n",
        "    outputs = self.decoder(self.dropout(encoding))                            # Implementation of Linear Layer. \n",
        "    return outputs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzwJ1cgFoyjz"
      },
      "source": [
        "#@ INITIALIZING TEXT CONVOLUTIONAL NEURAL NETWORKS: \n",
        "embed_size, kernel_sizes, num_channels = 100, [3, 4, 5], [100, 100, 100]      # Initializing Parameters. \n",
        "devices = d2l.try_all_gpus()                                                  # Initialization. \n",
        "net = TextCNN(len(vocab), embed_size, kernel_sizes, num_channels)             # Initializing Text CNN. \n",
        "#@ INITIALIZING WEIGHTS: \n",
        "def init_weights(m):                                                          # Function for Initializing Weights. \n",
        "  if type(m) in (nn.Linear, nn.Conv1d):\n",
        "    nn.init.xavier_uniform_(m.weight)                                         # Xavier Initialization. \n",
        "net.apply(init_weights);"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRGtbibCqffG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}